<<<<<<< Updated upstream
{
  "dataset_reader": {
    "type": "squad",
    "tokenizer" : {
      "word_splitter" : {
        "pos_tags" : true,
        "parse" : true
        }
    }
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      },
      "token_characters": {
        "type": "characters",
        "character_tokenizer": {
          "byte_encoding": "utf-8",
          "start_tokens": [259],
          "end_tokens": [260]
        }
      }
      "dep_head": {
          "type" : "dependency_head"
      }
    }
  },
  "train_data_path": "data/train_sample.json",
  "validation_data_path": "data/dev_sample.json",
  "model": {
    "type": "bidaf",
    "dependency_parse_head_transformer_layer":0,
    "text_field_embedder": {
      "tokens": {
        "type": "embedding",
        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",
        "embedding_dim": 100,
        "trainable": false
      },
      "token_characters": {
        "type": "character_encoding",
        "embedding": {
          "num_embeddings": 262,
          "embedding_dim": 16
        },
        "encoder": {
          "type": "cnn",
          "embedding_dim": 16,
          "num_filters": 100,
          "ngram_filter_sizes": [5]
        },
        "dropout": 0.2
      }
    },
    "num_highway_layers": 2,
    "phrase_layer": {
      "type": "stacked_self_attention",
      "input_dim": 200,
      "hidden_dim": 200,
      "feedforward_hidden_dim":128
      "projection_dim":128
      "num_attention_heads":4
      "use_positional_encoding":True
      "num_layers": 2,
      "dropout_prob": 0.2
      "residual_dropout_prob":0
      "attention_dropout_prob":0.2
    },
    "similarity_function": {
      "type": "linear",
      "combination": "x,y,x*y",
      "tensor_1_dim": 200,
      "tensor_2_dim": 200
    },
    "modeling_layer": {
      "type": "lstm",
      "bidirectional": true,
      "input_size": 800,
      "hidden_size": 100,
      "num_layers": 2,
      "dropout": 0.2
    },
    "span_end_encoder": {
      "type": "lstm",
      "bidirectional": true,
      "input_size": 1400,
      "hidden_size": 100,
      "num_layers": 1,
      "dropout": 0.2
    },
    "dropout": 0.2
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [["passage", "num_tokens"], ["question", "num_tokens"]],
    "batch_size": 40
  },

  "trainer": {
    "num_epochs": 20,
    "grad_norm": 5.0,
    "patience": 10,
    "validation_metric": "+em",
    "cuda_device": 0,
    "learning_rate_scheduler": {
      "type": "reduce_on_plateau",
      "factor": 0.5,
      "mode": "max",
      "patience": 2
    },
    "optimizer": {
      "type": "adam",
      "betas": [0.9, 0.9]
    }
  }
}
=======
diff --git a/.gitignore b/.gitignore
index 24be841..652d49d 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,4 +1,5 @@
 *.pyc
+data/*
 .cache/
 .coverage
 doc/_build/
@@ -11,3 +12,7 @@ allennlp.egg-info/
 build/
 dist/
 .mypy_cache
+python36/*
+output_path*/*
+run_logs/*
+examples.jsonl
diff --git a/allennlp/data/token_indexers/__init__.py b/allennlp/data/token_indexers/__init__.py
index 83eb79d..f6e228a 100644
--- a/allennlp/data/token_indexers/__init__.py
+++ b/allennlp/data/token_indexers/__init__.py
@@ -3,6 +3,7 @@ A ``TokenIndexer`` determines how string tokens get represented as arrays of ind
 """
 
 from allennlp.data.token_indexers.dep_label_indexer import DepLabelIndexer
+from allennlp.data.token_indexers.dep_head_indexer import DepHeadIndexer
 from allennlp.data.token_indexers.ner_tag_indexer import NerTagIndexer
 from allennlp.data.token_indexers.pos_tag_indexer import PosTagIndexer
 from allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer
diff --git a/allennlp/data/token_indexers/dep_head_indexer.py b/allennlp/data/token_indexers/dep_head_indexer.py
new file mode 100644
index 0000000..ca77253
--- /dev/null
+++ b/allennlp/data/token_indexers/dep_head_indexer.py
@@ -0,0 +1,66 @@
+import logging
+from typing import Dict, List, Set
+
+from overrides import overrides
+
+from allennlp.common.util import pad_sequence_to_length
+from allennlp.common import Params
+from allennlp.data.vocabulary import Vocabulary
+from allennlp.data.tokenizers.token import Token
+from allennlp.data.token_indexers.token_indexer import TokenIndexer
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+@TokenIndexer.register("dependency_head")
+class DepHeadIndexer(TokenIndexer[int]):
+    """
+    This :class:`TokenIndexer` represents tokens by their syntactic dependency head, as determined
+    by the ``head_`` field on ``Token``.
+
+    Parameters
+    ----------
+    namespace : ``str``, optional (default=``dep_heads``)
+        We will use this namespace in the :class:`Vocabulary` to map strings to indices.
+    """
+    # pylint: disable=no-self-use
+    def __init__(self, namespace: str = 'dep_heads') -> None:
+        self.namespace = namespace
+        self._logged_errors: Set[str] = set()
+
+    @overrides
+    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
+        dep_head = token.head.text
+        if not dep_head:
+            if token.text not in self._logged_errors:
+                logger.warning("Token had no dependency label: %s", token.text)
+                self._logged_errors.add(token.text)
+            dep_head = 'NONE'
+        counter[self.namespace][dep_head] += 1
+
+    @overrides
+    def token_to_indices(self, token: Token, vocabulary: Vocabulary) -> int:
+        dep_head = token.head or Token('NONE')
+        dep_head_indices = dep_head.i
+        return dep_head_indices
+
+    @overrides
+    def get_padding_token(self) -> int:
+        return 0
+
+    @overrides
+    def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument
+        return {}
+
+    @overrides
+    def pad_token_sequence(self,
+                           tokens: List[int],
+                           desired_num_tokens: int,
+                           padding_lengths: Dict[str, int]) -> List[int]:  # pylint: disable=unused-argument
+        return pad_sequence_to_length(tokens, desired_num_tokens)
+
+    @classmethod
+    def from_params(cls, params: Params) -> 'DepHeadIndexer':
+        namespace = params.pop('namespace', 'dep_heads')
+        params.assert_empty(cls.__name__)
+        return cls(namespace=namespace)
diff --git a/allennlp/data/tokenizers/token.py b/allennlp/data/tokenizers/token.py
index 4d02c03..4fe4bf9 100644
--- a/allennlp/data/tokenizers/token.py
+++ b/allennlp/data/tokenizers/token.py
@@ -38,7 +38,8 @@ class Token:
                  tag: str = None,
                  dep: str = None,
                  ent_type: str = None,
-                 text_id: int = None) -> None:
+                 text_id: int = None,
+                 head : str = None) -> None:
         self.text = text
         self.idx = idx
         self.lemma_ = lemma
@@ -47,6 +48,7 @@ class Token:
         self.dep_ = dep
         self.ent_type_ = ent_type
         self.text_id = text_id
+        self.head = head
 
     def __str__(self):
         return self.text
diff --git a/allennlp/models/reading_comprehension/bidaf.py b/allennlp/models/reading_comprehension/bidaf.py
index 4ab0b36..697d533 100644
--- a/allennlp/models/reading_comprehension/bidaf.py
+++ b/allennlp/models/reading_comprehension/bidaf.py
@@ -74,7 +74,8 @@ class BidirectionalAttentionFlow(Model):
                  dropout: float = 0.2,
                  mask_lstms: bool = True,
                  initializer: InitializerApplicator = InitializerApplicator(),
-                 regularizer: Optional[RegularizerApplicator] = None) -> None:
+                 regularizer: Optional[RegularizerApplicator] = None,
+                 lambdaa:int=None) -> None:
         super(BidirectionalAttentionFlow, self).__init__(vocab, regularizer)
 
         self._text_field_embedder = text_field_embedder
@@ -111,10 +112,13 @@ class BidirectionalAttentionFlow(Model):
             self._dropout = torch.nn.Dropout(p=dropout)
         else:
             self._dropout = lambda x: x
+        if lambdaa!=None:
+            self._lambda=int(lambdaa)
+        else: 
+            self._lambda=None
         self._mask_lstms = mask_lstms
 
         initializer(self)
-
     def forward(self,  # type: ignore
                 question: Dict[str, torch.LongTensor],
                 passage: Dict[str, torch.LongTensor],
@@ -171,8 +175,16 @@ class BidirectionalAttentionFlow(Model):
             string from the original passage that the model thinks is the best answer to the
             question.
         """
-        embedded_question = self._highway_layer(self._text_field_embedder(question))
-        embedded_passage = self._highway_layer(self._text_field_embedder(passage))
+
+        parse_attentionhead_layer = 0
+        ### Separate attention head labels
+        question_indices = {key: value for key, value in question.items() if key!='dep_head'}
+        passage_indices = {key: value for key, value in passage.items() if key!='dep_head'}
+
+        ### Make output
+        ## text embedding and highway layer
+        embedded_question = self._highway_layer(self._text_field_embedder(question_indices))
+        embedded_passage = self._highway_layer(self._text_field_embedder(passage_indices))
         batch_size = embedded_question.size(0)
         passage_length = embedded_passage.size(1)
         question_mask = util.get_text_field_mask(question).float()
@@ -180,10 +192,24 @@ class BidirectionalAttentionFlow(Model):
         question_lstm_mask = question_mask if self._mask_lstms else None
         passage_lstm_mask = passage_mask if self._mask_lstms else None
 
-        encoded_question = self._dropout(self._phrase_layer(embedded_question, question_lstm_mask))
-        encoded_passage = self._dropout(self._phrase_layer(embedded_passage, passage_lstm_mask))
+        ## phrase layer
+        phrase_layer_output_dict_question = self._phrase_layer(embedded_question, question_lstm_mask)
+        phrase_layer_output_question = phrase_layer_output_dict_question['output']
+        encoded_question = self._dropout(phrase_layer_output_question )
+        if 'attention' in phrase_layer_output_dict_question:
+            phrase_layer_attention_question = phrase_layer_output_dict_question['attention']
+
+        phrase_layer_output_dict_passage = self._phrase_layer(embedded_passage, passage_lstm_mask)
+        phrase_layer_output_passage  = phrase_layer_output_dict_passage ['output']
+        encoded_passage  = self._dropout(phrase_layer_output_passage )
+        if 'attention' in phrase_layer_output_dict_passage :
+            phrase_layer_attention_passage  = phrase_layer_output_dict_passage ['attention']
+
         encoding_dim = encoded_question.size(-1)
 
+        ## TODO - gold parse during training
+
+        ## bi-directional attention layer
         # Shape: (batch_size, passage_length, question_length)
         passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)
         # Shape: (batch_size, passage_length, question_length)
@@ -213,10 +239,11 @@ class BidirectionalAttentionFlow(Model):
                                           encoded_passage * passage_question_vectors,
                                           encoded_passage * tiled_question_passage_vector],
                                          dim=-1)
-
+        ## modeling layer
         modeled_passage = self._dropout(self._modeling_layer(final_merged_passage, passage_lstm_mask))
         modeling_dim = modeled_passage.size(-1)
 
+        ## start prediction layer
         # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))
         span_start_input = self._dropout(torch.cat([final_merged_passage, modeled_passage], dim=-1))
         # Shape: (batch_size, passage_length)
@@ -231,6 +258,7 @@ class BidirectionalAttentionFlow(Model):
                                                                                    passage_length,
                                                                                    modeling_dim)
 
+        ## end prediction layer
         # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim * 3)
         span_end_representation = torch.cat([final_merged_passage,
                                              modeled_passage,
@@ -248,6 +276,7 @@ class BidirectionalAttentionFlow(Model):
         span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)
         best_span = self.get_best_span(span_start_logits, span_end_logits)
 
+        ### make output dictionary
         output_dict = {
                 "passage_question_attention": passage_question_attention,
                 "span_start_logits": span_start_logits,
@@ -257,16 +286,43 @@ class BidirectionalAttentionFlow(Model):
                 "best_span": best_span,
                 }
 
-        # Compute the loss for training.
+        ### Compute the loss for training.
         if span_start is not None:
             loss = nll_loss(util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))
-            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))
             loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))
+            # Add dependency parse loss
+            # TODO : change for 'gold parse in train mode' to have the original dependency head predicitons, not those set to 1
+            batch_size = list(phrase_layer_output_question.size())[0]
+            if 'attention' in phrase_layer_output_dict_question:
+                gold_heads_question = question['dep_head']
+                gold_heads_passage = passage['dep_head']
+                for sample in range(batch_size):
+                    # Add loss for each token in question
+                    timesteps_question = list(phrase_layer_output_question.size())[1]
+                    
+                    for timestep_question in range(timesteps_question):
+                        gold_head_question = gold_heads_question[sample][timestep_question]
+                        gold_attention = phrase_layer_attention_question[parse_attentionhead_layer]\
+                          [sample][0][timestep_question][gold_head_question]
+                        loss += self._lambda*(1 - gold_attention)
+                    # Add loss for each token in passage
+                    timesteps_passage = list(phrase_layer_output_passage.size())[1]
+                    for timestep_passage in range(timesteps_passage):
+                        gold_head_passage = gold_heads_passage[sample][timestep_passage]
+                        gold_attention = phrase_layer_attention_passage[parse_attentionhead_layer]\
+                          [sample][0][timestep_passage][gold_head_passage]
+                        loss += self._lambda*(1 - gold_attention)
+
+
+            output_dict["loss"] = loss
+
+        ### Compute Accuracies
+            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))
             self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))
             self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))
-            output_dict["loss"] = loss
 
-        # Compute the EM and F1 on SQuAD and add the tokenized input to the output.
+
+        ### Compute the EM and F1 on SQuAD and add the tokenized input to the output.
         if metadata is not None:
             output_dict['best_span_str'] = []
             question_tokens = []
@@ -286,6 +342,7 @@ class BidirectionalAttentionFlow(Model):
                     self._squad_metrics(best_span_string, answer_texts)
             output_dict['question_tokens'] = question_tokens
             output_dict['passage_tokens'] = passage_tokens
+
         return output_dict
 
     def get_metrics(self, reset: bool = False) -> Dict[str, float]:
@@ -330,6 +387,7 @@ class BidirectionalAttentionFlow(Model):
     def from_params(cls, vocab: Vocabulary, params: Params) -> 'BidirectionalAttentionFlow':
         embedder_params = params.pop("text_field_embedder")
         text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)
+        lambdaa=params.pop("lambda")
         num_highway_layers = params.pop_int("num_highway_layers")
         phrase_layer = Seq2SeqEncoder.from_params(params.pop("phrase_layer"))
         similarity_function = SimilarityFunction.from_params(params.pop("similarity_function"))
@@ -343,6 +401,7 @@ class BidirectionalAttentionFlow(Model):
         mask_lstms = params.pop_bool('mask_lstms', True)
         params.assert_empty(cls.__name__)
         return cls(vocab=vocab,
+                   lambdaa=lambdaa,    
                    text_field_embedder=text_field_embedder,
                    num_highway_layers=num_highway_layers,
                    phrase_layer=phrase_layer,
diff --git a/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py b/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
index c47f204..2f2b3df 100644
--- a/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
+++ b/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
@@ -93,9 +93,12 @@ class MultiHeadSelfAttention(Seq2SeqEncoder):
 
         Returns
         -------
-        A tensor of shape (batch_size, timesteps, output_projection_dim),
+        An output dictionary consisting of:
+        attention : A tensor of shape(num_heads, batch_size, timesteps, timesteps)
+        output : A tensor of shape (batch_size, timesteps, output_projection_dim),
         where output_projection_dim = input_dim by default.
         """
+        output_dict = dict()
         num_heads = self._num_heads
 
         batch_size, timesteps, _ = inputs.size()
@@ -150,7 +153,12 @@ class MultiHeadSelfAttention(Seq2SeqEncoder):
         # Project back to original input size.
         # shape (batch_size, timesteps, input_size)
         outputs = self._output_projection(outputs)
-        return outputs
+
+        attention = attention.view(batch_size, num_heads, timesteps, timesteps)
+
+        output_dict['attention'] = attention
+        output_dict['output'] = outputs
+        return output_dict
 
     @classmethod
     def from_params(cls, params: Params) -> 'MultiHeadSelfAttention':
diff --git a/allennlp/modules/seq2seq_encoders/stacked_self_attention.py b/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
index 7406137..181a9bd 100644
--- a/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
+++ b/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
@@ -126,6 +126,15 @@ class StackedSelfAttentionEncoder(Seq2SeqEncoder):
 
     @overrides
     def forward(self, inputs: torch.Tensor, mask: torch.Tensor): # pylint: disable=arguments-differ
+        '''
+        Returns
+        -------
+        An output dictionary consisting of:
+        attention : A tensor of shape(num_layers, batch_size, num_heads, timesteps, timesteps)
+        output :
+        '''
+        output_dict = dict()
+        output_dict['attention'] = []
         if self._use_positional_encoding:
             output = add_positional_features(inputs)
         else:
@@ -148,10 +157,14 @@ class StackedSelfAttentionEncoder(Seq2SeqEncoder):
                 # layers, so we exclude it here.
                 feedforward_output = feedforward_layer_norm(feedforward_output + cached_input)
             # shape (batch_size, sequence_length, hidden_dim)
-            attention_output = attention(feedforward_output, mask)
+            attention_layer_output = attention(feedforward_output, mask)
+            attention_output = attention_layer_output['output']
+            attention = attention_layer_output['attention']
+            output_dict['attention'].append(attention)
             output = layer_norm(self.dropout(attention_output) + feedforward_output)
+        output_dict['output'] = output
 
-        return output
+        return output_dict
 
     @classmethod
     def from_params(cls, params: Params):
diff --git a/srun.sh b/srun.sh
new file mode 100644
index 0000000..6b8a7d6
--- /dev/null
+++ b/srun.sh
@@ -0,0 +1,31 @@
+#!/bin/bash
+#SBATCH --mem=40GB
+#SBATCH --job-name=2-gpu-bidaf-pytorch
+#SBATCH --output=run_logs/res_%j.txt 	# output file
+#SBATCH -e run_logs/res_%j.err        	# File to which STDERR will be written
+#
+#SBATCH --gres=gpu:2
+#SBATCH --mail-type=ALL
+#SBATCH --mail-user=shasvatmukes@cs.umass.edu
+echo $SLURM_JOBID - `hostname` >> ~/slurm-jobs.txt
+
+module purge
+module load python/3.6.1
+module load cuda80/blas/8.0.44
+module load cuda80/fft/8.0.44
+module load cuda80/nsight/8.0.44
+module load cuda80/profiler/8.0.44
+module load cuda80/toolkit/8.0.44
+
+## Change this line so that it points to your bidaf github folder
+
+# Training (Default - on SQuAD)
+rm -rf output_path_parser
+python -m allennlp.run train training_config/bidaf_parser.json -s output_path_parser
+
+# Evaluation (Default - on SQuAD)
+# python -m allennlp.run evaluate output_path --evaluation-data-file data/val.json
+
+# Prediction on a user defined passage
+# echo '{"passage": "A reusable launch system (RLS, or reusable launch vehicle, RLV) is a launch system which is capable of launching a payload into space more than once. This contrasts with expendable launch systems, where each launch vehicle is launched once and then discarded. No completely reusable orbital launch system has ever been created. Two partially reusable launch systems were developed, the Space Shuttle and Falcon 9. The Space Shuttle was partially reusable: the orbiter (which included the Space Shuttle main engines and the Orbital Maneuvering System engines), and the two solid rocket boosters were reused after several months of refitting work for each launch. The external tank was discarded after each flight.", "question": "How many partially reusable launch systems were developed?"}' > examples.jsonl
+# python -m allennlp.run predict output_path examples.jsonl
diff --git a/training_config/bidaf__serialization_dir.json b/training_config/bidaf__serialization_dir.json
new file mode 100644
index 0000000..0947f47
--- /dev/null
+++ b/training_config/bidaf__serialization_dir.json
@@ -0,0 +1,102 @@
+{
+	"serialization_dir": 'store/bidaf/',
+  "dataset_reader": {
+    "type": "squad",
+    "token_indexers": {
+      "tokens": {
+        "type": "single_id",
+        "lowercase_tokens": true
+      },
+      "token_characters": {
+        "type": "characters",
+        "character_tokenizer": {
+          "byte_encoding": "utf-8",
+          "start_tokens": [259],
+          "end_tokens": [260]
+        }
+      }
+    }
+  },
+  "train_data_path": "data/train.json",
+  "validation_data_path": "data/dev.json",
+  "model": {
+    "type": "bidaf",
+    "text_field_embedder": {
+      "tokens": {
+        "type": "embedding",
+        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",
+        "embedding_dim": 100,
+        "trainable": false
+      },
+      "token_characters": {
+        "type": "character_encoding",
+        "embedding": {
+          "num_embeddings": 262,
+          "embedding_dim": 16
+        },
+        "encoder": {
+          "type": "cnn",
+          "embedding_dim": 16,
+          "num_filters": 100,
+          "ngram_filter_sizes": [5]
+        },
+        "dropout": 0.2
+      }
+    },
+    "num_highway_layers": 2,
+    "phrase_layer": {
+      "type": "lstm",
+      "bidirectional": true,
+      "input_size": 200,
+      "hidden_size": 100,
+      "num_layers": 1,
+      "dropout": 0.2
+    },
+    "similarity_function": {
+      "type": "linear",
+      "combination": "x,y,x*y",
+      "tensor_1_dim": 200,
+      "tensor_2_dim": 200
+    },
+    "modeling_layer": {
+      "type": "lstm",
+      "bidirectional": true,
+      "input_size": 800,
+      "hidden_size": 100,
+      "num_layers": 2,
+      "dropout": 0.2
+    },
+    "span_end_encoder": {
+      "type": "lstm",
+      "bidirectional": true,
+      "input_size": 1400,
+      "hidden_size": 100,
+      "num_layers": 1,
+      "dropout": 0.2
+    },
+    "dropout": 0.2
+  },
+  "iterator": {
+    "type": "bucket",
+    "sorting_keys": [["passage", "num_tokens"], ["question", "num_tokens"]],
+    "batch_size": 40
+  },
+
+  "trainer": {
+    "num_epochs": 1,
+    "grad_norm": 5.0,
+    "patience": 10,
+    "validation_metric": "+em",
+    "cuda_device": 0,
+    "learning_rate_scheduler": {
+      "type": "reduce_on_plateau",
+      "factor": 0.5,
+      "mode": "max",
+      "patience": 2
+    },
+    "optimizer": {
+      "type": "adam",
+      "betas": [0.9, 0.9]
+    }
+  }
+}
diff --git a/training_config/bidaf_parser.json b/training_config/bidaf_parser.json
new file mode 100644
index 0000000..31ccb01
--- /dev/null
+++ b/training_config/bidaf_parser.json
@@ -0,0 +1,444 @@
+diff --git a/.gitignore b/.gitignore
+index 24be841..652d49d 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -1,4 +1,5 @@
+ *.pyc
++data/*
+ .cache/
+ .coverage
+ doc/_build/
+@@ -11,3 +12,7 @@ allennlp.egg-info/
+ build/
+ dist/
+ .mypy_cache
++python36/*
++output_path*/*
++run_logs/*
++examples.jsonl
+diff --git a/allennlp/data/token_indexers/__init__.py b/allennlp/data/token_indexers/__init__.py
+index 83eb79d..f6e228a 100644
+--- a/allennlp/data/token_indexers/__init__.py
++++ b/allennlp/data/token_indexers/__init__.py
+@@ -3,6 +3,7 @@ A ``TokenIndexer`` determines how string tokens get represented as arrays of ind
+ """
+ 
+ from allennlp.data.token_indexers.dep_label_indexer import DepLabelIndexer
++from allennlp.data.token_indexers.dep_head_indexer import DepHeadIndexer
+ from allennlp.data.token_indexers.ner_tag_indexer import NerTagIndexer
+ from allennlp.data.token_indexers.pos_tag_indexer import PosTagIndexer
+ from allennlp.data.token_indexers.single_id_token_indexer import SingleIdTokenIndexer
+diff --git a/allennlp/data/token_indexers/dep_head_indexer.py b/allennlp/data/token_indexers/dep_head_indexer.py
+new file mode 100644
+index 0000000..ca77253
+--- /dev/null
++++ b/allennlp/data/token_indexers/dep_head_indexer.py
+@@ -0,0 +1,66 @@
++import logging
++from typing import Dict, List, Set
++
++from overrides import overrides
++
++from allennlp.common.util import pad_sequence_to_length
++from allennlp.common import Params
++from allennlp.data.vocabulary import Vocabulary
++from allennlp.data.tokenizers.token import Token
++from allennlp.data.token_indexers.token_indexer import TokenIndexer
++
++logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
++
++
++@TokenIndexer.register("dependency_head")
++class DepHeadIndexer(TokenIndexer[int]):
++    """
++    This :class:`TokenIndexer` represents tokens by their syntactic dependency head, as determined
++    by the ``head_`` field on ``Token``.
++
++    Parameters
++    ----------
++    namespace : ``str``, optional (default=``dep_heads``)
++        We will use this namespace in the :class:`Vocabulary` to map strings to indices.
++    """
++    # pylint: disable=no-self-use
++    def __init__(self, namespace: str = 'dep_heads') -> None:
++        self.namespace = namespace
++        self._logged_errors: Set[str] = set()
++
++    @overrides
++    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):
++        dep_head = token.head.text
++        if not dep_head:
++            if token.text not in self._logged_errors:
++                logger.warning("Token had no dependency label: %s", token.text)
++                self._logged_errors.add(token.text)
++            dep_head = 'NONE'
++        counter[self.namespace][dep_head] += 1
++
++    @overrides
++    def token_to_indices(self, token: Token, vocabulary: Vocabulary) -> int:
++        dep_head = token.head or Token('NONE')
++        dep_head_indices = dep_head.i
++        return dep_head_indices
++
++    @overrides
++    def get_padding_token(self) -> int:
++        return 0
++
++    @overrides
++    def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument
++        return {}
++
++    @overrides
++    def pad_token_sequence(self,
++                           tokens: List[int],
++                           desired_num_tokens: int,
++                           padding_lengths: Dict[str, int]) -> List[int]:  # pylint: disable=unused-argument
++        return pad_sequence_to_length(tokens, desired_num_tokens)
++
++    @classmethod
++    def from_params(cls, params: Params) -> 'DepHeadIndexer':
++        namespace = params.pop('namespace', 'dep_heads')
++        params.assert_empty(cls.__name__)
++        return cls(namespace=namespace)
+diff --git a/allennlp/data/tokenizers/token.py b/allennlp/data/tokenizers/token.py
+index 4d02c03..4fe4bf9 100644
+--- a/allennlp/data/tokenizers/token.py
++++ b/allennlp/data/tokenizers/token.py
+@@ -38,7 +38,8 @@ class Token:
+                  tag: str = None,
+                  dep: str = None,
+                  ent_type: str = None,
+-                 text_id: int = None) -> None:
++                 text_id: int = None,
++                 head : str = None) -> None:
+         self.text = text
+         self.idx = idx
+         self.lemma_ = lemma
+@@ -47,6 +48,7 @@ class Token:
+         self.dep_ = dep
+         self.ent_type_ = ent_type
+         self.text_id = text_id
++        self.head = head
+ 
+     def __str__(self):
+         return self.text
+diff --git a/allennlp/models/reading_comprehension/bidaf.py b/allennlp/models/reading_comprehension/bidaf.py
+index 4ab0b36..697d533 100644
+--- a/allennlp/models/reading_comprehension/bidaf.py
++++ b/allennlp/models/reading_comprehension/bidaf.py
+@@ -74,7 +74,8 @@ class BidirectionalAttentionFlow(Model):
+                  dropout: float = 0.2,
+                  mask_lstms: bool = True,
+                  initializer: InitializerApplicator = InitializerApplicator(),
+-                 regularizer: Optional[RegularizerApplicator] = None) -> None:
++                 regularizer: Optional[RegularizerApplicator] = None,
++                 lambdaa:int=None) -> None:
+         super(BidirectionalAttentionFlow, self).__init__(vocab, regularizer)
+ 
+         self._text_field_embedder = text_field_embedder
+@@ -111,10 +112,13 @@ class BidirectionalAttentionFlow(Model):
+             self._dropout = torch.nn.Dropout(p=dropout)
+         else:
+             self._dropout = lambda x: x
++        if lambdaa!=None:
++            self._lambda=int(lambdaa)
++        else: 
++            self._lambda=None
+         self._mask_lstms = mask_lstms
+ 
+         initializer(self)
+-
+     def forward(self,  # type: ignore
+                 question: Dict[str, torch.LongTensor],
+                 passage: Dict[str, torch.LongTensor],
+@@ -171,8 +175,16 @@ class BidirectionalAttentionFlow(Model):
+             string from the original passage that the model thinks is the best answer to the
+             question.
+         """
+-        embedded_question = self._highway_layer(self._text_field_embedder(question))
+-        embedded_passage = self._highway_layer(self._text_field_embedder(passage))
++
++        parse_attentionhead_layer = 0
++        ### Separate attention head labels
++        question_indices = {key: value for key, value in question.items() if key!='dep_head'}
++        passage_indices = {key: value for key, value in passage.items() if key!='dep_head'}
++
++        ### Make output
++        ## text embedding and highway layer
++        embedded_question = self._highway_layer(self._text_field_embedder(question_indices))
++        embedded_passage = self._highway_layer(self._text_field_embedder(passage_indices))
+         batch_size = embedded_question.size(0)
+         passage_length = embedded_passage.size(1)
+         question_mask = util.get_text_field_mask(question).float()
+@@ -180,10 +192,24 @@ class BidirectionalAttentionFlow(Model):
+         question_lstm_mask = question_mask if self._mask_lstms else None
+         passage_lstm_mask = passage_mask if self._mask_lstms else None
+ 
+-        encoded_question = self._dropout(self._phrase_layer(embedded_question, question_lstm_mask))
+-        encoded_passage = self._dropout(self._phrase_layer(embedded_passage, passage_lstm_mask))
++        ## phrase layer
++        phrase_layer_output_dict_question = self._phrase_layer(embedded_question, question_lstm_mask)
++        phrase_layer_output_question = phrase_layer_output_dict_question['output']
++        encoded_question = self._dropout(phrase_layer_output_question )
++        if 'attention' in phrase_layer_output_dict_question:
++            phrase_layer_attention_question = phrase_layer_output_dict_question['attention']
++
++        phrase_layer_output_dict_passage = self._phrase_layer(embedded_passage, passage_lstm_mask)
++        phrase_layer_output_passage  = phrase_layer_output_dict_passage ['output']
++        encoded_passage  = self._dropout(phrase_layer_output_passage )
++        if 'attention' in phrase_layer_output_dict_passage :
++            phrase_layer_attention_passage  = phrase_layer_output_dict_passage ['attention']
++
+         encoding_dim = encoded_question.size(-1)
+ 
++        ## TODO - gold parse during training
++
++        ## bi-directional attention layer
+         # Shape: (batch_size, passage_length, question_length)
+         passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)
+         # Shape: (batch_size, passage_length, question_length)
+@@ -213,10 +239,11 @@ class BidirectionalAttentionFlow(Model):
+                                           encoded_passage * passage_question_vectors,
+                                           encoded_passage * tiled_question_passage_vector],
+                                          dim=-1)
+-
++        ## modeling layer
+         modeled_passage = self._dropout(self._modeling_layer(final_merged_passage, passage_lstm_mask))
+         modeling_dim = modeled_passage.size(-1)
+ 
++        ## start prediction layer
+         # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))
+         span_start_input = self._dropout(torch.cat([final_merged_passage, modeled_passage], dim=-1))
+         # Shape: (batch_size, passage_length)
+@@ -231,6 +258,7 @@ class BidirectionalAttentionFlow(Model):
+                                                                                    passage_length,
+                                                                                    modeling_dim)
+ 
++        ## end prediction layer
+         # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim * 3)
+         span_end_representation = torch.cat([final_merged_passage,
+                                              modeled_passage,
+@@ -248,6 +276,7 @@ class BidirectionalAttentionFlow(Model):
+         span_end_logits = util.replace_masked_values(span_end_logits, passage_mask, -1e7)
+         best_span = self.get_best_span(span_start_logits, span_end_logits)
+ 
++        ### make output dictionary
+         output_dict = {
+                 "passage_question_attention": passage_question_attention,
+                 "span_start_logits": span_start_logits,
+@@ -257,16 +286,43 @@ class BidirectionalAttentionFlow(Model):
+                 "best_span": best_span,
+                 }
+ 
+-        # Compute the loss for training.
++        ### Compute the loss for training.
+         if span_start is not None:
+             loss = nll_loss(util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1))
+-            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))
+             loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))
++            # Add dependency parse loss
++            # TODO : change for 'gold parse in train mode' to have the original dependency head predicitons, not those set to 1
++            batch_size = list(phrase_layer_output_question.size())[0]
++            if 'attention' in phrase_layer_output_dict_question:
++                gold_heads_question = question['dep_head']
++                gold_heads_passage = passage['dep_head']
++                for sample in range(batch_size):
++                    # Add loss for each token in question
++                    timesteps_question = list(phrase_layer_output_question.size())[1]
++                    
++                    for timestep_question in range(timesteps_question):
++                        gold_head_question = gold_heads_question[sample][timestep_question]
++                        gold_attention = phrase_layer_attention_question[parse_attentionhead_layer]\
++                          [sample][0][timestep_question][gold_head_question]
++                        loss += self._lambda*(1 - gold_attention)
++                    # Add loss for each token in passage
++                    timesteps_passage = list(phrase_layer_output_passage.size())[1]
++                    for timestep_passage in range(timesteps_passage):
++                        gold_head_passage = gold_heads_passage[sample][timestep_passage]
++                        gold_attention = phrase_layer_attention_passage[parse_attentionhead_layer]\
++                          [sample][0][timestep_passage][gold_head_passage]
++                        loss += self._lambda*(1 - gold_attention)
++
++
++            output_dict["loss"] = loss
++
++        ### Compute Accuracies
++            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))
+             self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))
+             self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))
+-            output_dict["loss"] = loss
+ 
+-        # Compute the EM and F1 on SQuAD and add the tokenized input to the output.
++
++        ### Compute the EM and F1 on SQuAD and add the tokenized input to the output.
+         if metadata is not None:
+             output_dict['best_span_str'] = []
+             question_tokens = []
+@@ -286,6 +342,7 @@ class BidirectionalAttentionFlow(Model):
+                     self._squad_metrics(best_span_string, answer_texts)
+             output_dict['question_tokens'] = question_tokens
+             output_dict['passage_tokens'] = passage_tokens
++
+         return output_dict
+ 
+     def get_metrics(self, reset: bool = False) -> Dict[str, float]:
+@@ -330,6 +387,7 @@ class BidirectionalAttentionFlow(Model):
+     def from_params(cls, vocab: Vocabulary, params: Params) -> 'BidirectionalAttentionFlow':
+         embedder_params = params.pop("text_field_embedder")
+         text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)
++        lambdaa=params.pop("lambda")
+         num_highway_layers = params.pop_int("num_highway_layers")
+         phrase_layer = Seq2SeqEncoder.from_params(params.pop("phrase_layer"))
+         similarity_function = SimilarityFunction.from_params(params.pop("similarity_function"))
+@@ -343,6 +401,7 @@ class BidirectionalAttentionFlow(Model):
+         mask_lstms = params.pop_bool('mask_lstms', True)
+         params.assert_empty(cls.__name__)
+         return cls(vocab=vocab,
++                   lambdaa=lambdaa,    
+                    text_field_embedder=text_field_embedder,
+                    num_highway_layers=num_highway_layers,
+                    phrase_layer=phrase_layer,
+diff --git a/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py b/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
+index c47f204..2f2b3df 100644
+--- a/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
++++ b/allennlp/modules/seq2seq_encoders/multi_head_self_attention.py
+@@ -93,9 +93,12 @@ class MultiHeadSelfAttention(Seq2SeqEncoder):
+ 
+         Returns
+         -------
+-        A tensor of shape (batch_size, timesteps, output_projection_dim),
++        An output dictionary consisting of:
++        attention : A tensor of shape(num_heads, batch_size, timesteps, timesteps)
++        output : A tensor of shape (batch_size, timesteps, output_projection_dim),
+         where output_projection_dim = input_dim by default.
+         """
++        output_dict = dict()
+         num_heads = self._num_heads
+ 
+         batch_size, timesteps, _ = inputs.size()
+@@ -150,7 +153,12 @@ class MultiHeadSelfAttention(Seq2SeqEncoder):
+         # Project back to original input size.
+         # shape (batch_size, timesteps, input_size)
+         outputs = self._output_projection(outputs)
+-        return outputs
++
++        attention = attention.view(batch_size, num_heads, timesteps, timesteps)
++
++        output_dict['attention'] = attention
++        output_dict['output'] = outputs
++        return output_dict
+ 
+     @classmethod
+     def from_params(cls, params: Params) -> 'MultiHeadSelfAttention':
+diff --git a/allennlp/modules/seq2seq_encoders/stacked_self_attention.py b/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
+index 7406137..181a9bd 100644
+--- a/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
++++ b/allennlp/modules/seq2seq_encoders/stacked_self_attention.py
+@@ -126,6 +126,15 @@ class StackedSelfAttentionEncoder(Seq2SeqEncoder):
+ 
+     @overrides
+     def forward(self, inputs: torch.Tensor, mask: torch.Tensor): # pylint: disable=arguments-differ
++        '''
++        Returns
++        -------
++        An output dictionary consisting of:
++        attention : A tensor of shape(num_layers, batch_size, num_heads, timesteps, timesteps)
++        output :
++        '''
++        output_dict = dict()
++        output_dict['attention'] = []
+         if self._use_positional_encoding:
+             output = add_positional_features(inputs)
+         else:
+@@ -148,10 +157,14 @@ class StackedSelfAttentionEncoder(Seq2SeqEncoder):
+                 # layers, so we exclude it here.
+                 feedforward_output = feedforward_layer_norm(feedforward_output + cached_input)
+             # shape (batch_size, sequence_length, hidden_dim)
+-            attention_output = attention(feedforward_output, mask)
++            attention_layer_output = attention(feedforward_output, mask)
++            attention_output = attention_layer_output['output']
++            attention = attention_layer_output['attention']
++            output_dict['attention'].append(attention)
+             output = layer_norm(self.dropout(attention_output) + feedforward_output)
++        output_dict['output'] = output
+ 
+-        return output
++        return output_dict
+ 
+     @classmethod
+     def from_params(cls, params: Params):
+diff --git a/srun.sh b/srun.sh
+new file mode 100644
+index 0000000..6b8a7d6
+--- /dev/null
++++ b/srun.sh
+@@ -0,0 +1,31 @@
++#!/bin/bash
++#SBATCH --mem=40GB
++#SBATCH --job-name=2-gpu-bidaf-pytorch
++#SBATCH --output=run_logs/res_%j.txt 	# output file
++#SBATCH -e run_logs/res_%j.err        	# File to which STDERR will be written
++#
++#SBATCH --gres=gpu:2
++#SBATCH --mail-type=ALL
++#SBATCH --mail-user=shasvatmukes@cs.umass.edu
++echo $SLURM_JOBID - `hostname` >> ~/slurm-jobs.txt
++
++module purge
++module load python/3.6.1
++module load cuda80/blas/8.0.44
++module load cuda80/fft/8.0.44
++module load cuda80/nsight/8.0.44
++module load cuda80/profiler/8.0.44
++module load cuda80/toolkit/8.0.44
++
++## Change this line so that it points to your bidaf github folder
++
++# Training (Default - on SQuAD)
++rm -rf output_path_parser
++python -m allennlp.run train training_config/bidaf_parser.json -s output_path_parser
++
++# Evaluation (Default - on SQuAD)
++# python -m allennlp.run evaluate output_path --evaluation-data-file data/val.json
++
++# Prediction on a user defined passage
++# echo '{"passage": "A reusable launch system (RLS, or reusable launch vehicle, RLV) is a launch system which is capable of launching a payload into space more than once. This contrasts with expendable launch systems, where each launch vehicle is launched once and then discarded. No completely reusable orbital launch system has ever been created. Two partially reusable launch systems were developed, the Space Shuttle and Falcon 9. The Space Shuttle was partially reusable: the orbiter (which included the Space Shuttle main engines and the Orbital Maneuvering System engines), and the two solid rocket boosters were reused after several months of refitting work for each launch. The external tank was discarded after each flight.", "question": "How many partially reusable launch systems were developed?"}' > examples.jsonl
++# python -m allennlp.run predict output_path examples.jsonl
+diff --git a/training_config/bidaf__serialization_dir.json b/training_config/bidaf__serialization_dir.json
+new file mode 100644
+index 0000000..0947f47
+--- /dev/null
++++ b/training_config/bidaf__serialization_dir.json
+@@ -0,0 +1,102 @@
++{
++	"serialization_dir": 'store/bidaf/',
++  "dataset_reader": {
++    "type": "squad",
++    "token_indexers": {
++      "tokens": {
++        "type": "single_id",
++        "lowercase_tokens": true
++      },
++      "token_characters": {
++        "type": "characters",
++        "character_tokenizer": {
++          "byte_encoding": "utf-8",
++          "start_tokens": [259],
++          "end_tokens": [260]
++        }
++      }
++    }
++  },
++  "train_data_path": "data/train.json",
++  "validation_data_path": "data/dev.json",
++  "model": {
++    "type": "bidaf",
++    "text_field_embedder": {
++      "tokens": {
++        "type": "embedding",
++        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",
++        "embedding_dim": 100,
++        "trainable": false
++      },
++      "token_characters": {
++        "type": "character_
\ No newline at end of file
>>>>>>> Stashed changes
